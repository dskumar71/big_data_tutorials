{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>1) Read Data </H1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.1 Spark Natively reads the CSV file into Dataframe . Create Weather_org_df ( Weather original Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "weather_org_df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  org.apache.spark._ \n",
    "val weather_org_df = spark.\n",
    "        read.\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"mode\",\"DROPMALFORMED\").\n",
    "        option(\"delimiter\", \",\").\n",
    "        csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Check the Schema of the Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_org_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3  Create temp table Aus_Rain_Data called for easy data manipulation operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df.registerTempTable(\"Aus_Rain_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Try Reading some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://9c12c0ba0efc:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590298426800)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 05:33:44,299 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Table or view not found: Aus_Rain_Data; line 1 pos 14",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Table or view not found: Aus_Rain_Data; line 1 pos 14",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)",
      "  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:84)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)",
      "  ... 37 elided",
      "Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'aus_rain_data' not found in database 'default';",
      "  at org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)",
      "  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)",
      "  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)",
      "  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)",
      "  ... 79 more",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Aus_Rain_Data\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>2) Data Analysis  </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 How many locations the weather is recorded to predict rain the different zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df.select(mean(weather_org_df(\"MaxTemp\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: value AND is not a member of org.apache.spark.sql.Column",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: value AND is not a member of org.apache.spark.sql.Column",
      "       weather_org_df.select(\"MinTemp\").filter(weather_org_df(\"MinTemp\") < 0 AND weather_org_df(\"Location\") = Albury) .show()",
      "                                                                             ^",
      "<console>:31: error: not found: value Albury",
      "       weather_org_df.select(\"MinTemp\").filter(weather_org_df(\"MinTemp\") < 0 AND weather_org_df(\"Location\") = Albury) .show()",
      "                                                                                                              ^",
      ""
     ]
    }
   ],
   "source": [
    "weather_org_df.select(\"MinTemp\").filter(weather_org_df(\"MinTemp\") < 0 AND weather_org_df(\"Location\") = Albury) .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT location)|\n",
      "+------------------------+\n",
      "|                      49|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(distinct location) from Aus_Rain_Data \").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data contains weather data collected from 49 Stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|MinTemp|\n",
      "+-------+\n",
      "|   13.4|\n",
      "|    7.4|\n",
      "|   12.9|\n",
      "|    9.2|\n",
      "|   17.5|\n",
      "|   14.6|\n",
      "|   14.3|\n",
      "|    7.7|\n",
      "|    9.7|\n",
      "|   13.1|\n",
      "|   13.4|\n",
      "|   15.9|\n",
      "|   15.9|\n",
      "|   12.6|\n",
      "|    9.8|\n",
      "|   14.1|\n",
      "|   13.5|\n",
      "|   11.2|\n",
      "|    9.8|\n",
      "|   11.5|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+\n",
      "|missingValues|\n",
      "+-------------+\n",
      "|          637|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"select MinTemp  from Aus_Rain_Data where Date between '2008-12-01' and '2009-01-01' and Location = 'Albury' \").show()\n",
    "spark.sql(\"select count(MinTemp) as missingValues  from Aus_Rain_Data where MinTemp = 'NA' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|Location|avg(CAST(MinTemp AS DOUBLE))|\n",
      "+--------+----------------------------+\n",
      "|  Albury|           14.54331983805668|\n",
      "|  Albury|            5.83203463203463|\n",
      "|  Albury|           9.502083333333333|\n",
      "|  Albury|          12.552916666666665|\n",
      "|  Albury|          3.6544354838709703|\n",
      "|  Albury|           5.455434782608697|\n",
      "|  Albury|           16.74657534246576|\n",
      "|  Albury|           3.435984848484848|\n",
      "|  Albury|           8.321810699588474|\n",
      "|  Albury|          16.874193548387098|\n",
      "|  Albury|          13.721691176470591|\n",
      "|  Albury|          3.7752032520325205|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Location, AVG(MinTemp)  from Aus_Rain_Data  where location = 'Albury' group by Location, MONTH(date) \").show(50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For each location calculate avaerage temp for each month and substitute this for the row having NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|Location|avg(CAST(MinTemp AS DOUBLE))|\n",
      "+--------+----------------------------+\n",
      "|  Sydney|          14.865056988602275|\n",
      "|  Albury|            9.52089850249585|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "p1: String = ('1','2','3','4','5','6','7','8','9','10','11','12')\n",
       "p2: String = ('Albury','Sydney')\n",
       "query: String = select Location,AVG(MinTemp) from Aus_Rain_Data where Location in ('Albury','Sydney')   group by Location\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val p1 = \"('1','2','3','4','5','6','7','8','9','10','11','12')\"\n",
    "val p2 = \"('Albury','Sydney')\"\n",
    "\n",
    "val query  = s\"select Location,AVG(MinTemp) from Aus_Rain_Data where Location in $p2   group by Location\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: unclosed character literal",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: unclosed character literal",
      "weather_org_df = weather_org_df.withColumn(\"MinTemp\", F.when(F.col(\"MinTemp\")=='NA', 0).otherwise(F.col(\"MinTemp\")))",
      "                                                                                  ^",
      "<console>:2: error: ';' expected but ')' found.",
      "weather_org_df = weather_org_df.withColumn(\"MinTemp\", F.when(F.col(\"MinTemp\")=='NA', 0).otherwise(F.col(\"MinTemp\")))",
      "                                                                                                                   ^",
      ""
     ]
    }
   ],
   "source": [
    "weather_org_df = weather_org_df.withColumn(\"MinTemp\", F.when(F.col(\"MinTemp\")=='NA', 0).otherwise(F.col(\"MinTemp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: missing argument list for method $ in class StringToColumn",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: missing argument list for method $ in class StringToColumn",
      "Unapplied methods are only converted to functions when a function type is expected.",
      "You can make this conversion explicit by writing `$ _` or `$(_)` instead of `$`.",
      "       weather_org_df.withColumn(\"MinTempA\", when($\"MinTemp\"= NA, weather_org_df.select(mean(\"MinTemp\"))",
      "                                                  ^",
      "<console>:31: error: not found: value NA",
      "       weather_org_df.withColumn(\"MinTempA\", when($\"MinTemp\"= NA, weather_org_df.select(mean(\"MinTemp\"))",
      "                                                              ^",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "weather_org_df.withColumn(\"MinTempA\", when($\"MinTemp\"= NA, weather_org_df.select(mean(\"MinTemp\"))\n",
    "  .first()(0).asInstanceOf[Double])\n",
    "  .otherwise($\"MinTemp\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df.registerTempTable(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|MinTemp|\n",
      "+-------+\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select MinTemp from Test where Mintemp = 'NA' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: overloaded method value fill with alternatives:",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: overloaded method value fill with alternatives:",
      "  (value: Boolean,cols: Array[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: Boolean,cols: Seq[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: String,cols: Seq[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: String,cols: Array[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: Double,cols: Seq[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: Long,cols: Seq[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: Double,cols: Array[String])org.apache.spark.sql.DataFrame <and>",
      "  (value: Long,cols: Array[String])org.apache.spark.sql.DataFrame",
      " cannot be applied to (org.apache.spark.sql.DataFrame, Seq[String])",
      "       weather_org_df.na.fill(weather_org_df.select(mean(\"MinTemp\")),Seq(\"MinTemp\"))",
      "                         ^",
      ""
     ]
    }
   ],
   "source": [
    "weather_org_df.na.fill(weather_org_df.select(mean(\"MinTemp\")),Seq(\"MinTemp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: value isnan is not a member of org.apache.spark.sql.ColumnName",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: value isnan is not a member of org.apache.spark.sql.ColumnName",
      "       weather_org_df.withColumn(\"MinTempA\", when($\"MinTemp\".isnan, weather_org_df.select(mean(\"MinTemp\"))",
      "                                                             ^",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "weather_org_df.withColumn(\"MinTempA\", when($\"MinTemp\".isnull, weather_org_df.select(mean(\"MinTemp\"))\n",
    "  .first()(0).asInstanceOf[Double])\n",
    "  .otherwise($\"MinTemp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "31: error: value NA is not a member of org.apache.spark.sql.DataFrame",
     "output_type": "error",
     "traceback": [
      "<console>:31: error: value NA is not a member of org.apache.spark.sql.DataFrame",
      "       weather_org_df.NA.fill(0,Seq(\"MinTemp\"))",
      "                      ^",
      ""
     ]
    }
   ],
   "source": [
    "weather_org_df.NA.fill(0,Seq(\"MinTemp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|MinTemp|\n",
      "+-------+\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "|     NA|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select MinTemp from Test where Mintemp = 'NA' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
