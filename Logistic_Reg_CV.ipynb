{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://64fd13194e1e:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591615976164)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 11:32:49,920 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "//Australia having very distinct 8 climate zones,no single model will work for all australia \n",
    "// Even within a state  there are multiple climate zones(from alpine to high humid) \n",
    "//Different models need to be trained for different locations\n",
    "//In this approach,separate models will be created for different locations and saved in the directory\n",
    "//for the prediction particular location model will be loaded and prediction will be done\n",
    "//Data used was weatherAUS_YN.csv(before imputing the  null values) as it was  observed  that \n",
    "//dropping null values are more effective than imputing (In our scenario imputing means blunting the features i.e.  generalising the data)\n",
    "//Wind direction (categorical columns) not tested or used in the modelling as too much of encoding required and crashing the docker\n",
    "//correlation matrix(wrangling file)& trial & Error methods were used to select the features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator, CrossValidatorModel}\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator,CrossValidatorModel}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@577c468\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").\n",
    "    option(\"inferSchema\",\"true\").\n",
    "    format(\"csv\").\n",
    "    load(\"/home/weatherAUS-clean-FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- RainToday_Sc: integer (nullable = true)\n",
      " |-- RainTomorrow_Sc: integer (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Pressure3pm_Sc: double (nullable = true)\n",
      " |-- Pressure9am_Sc: double (nullable = true)\n",
      " |-- date_Sc: timestamp (nullable = true)\n",
      " |-- date-day_Sc: integer (nullable = true)\n",
      " |-- date-month_Sc: integer (nullable = true)\n",
      " |-- date-year_Sc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Selecting required fields with right data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "//based on correlation matrix following filelds were selected influencing the rainfall on the following day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = data.selectExpr(\"Date\",\"Location\",\n",
    "                        \"MaxTemp\",\"Humidity3pm\", \n",
    "                         \"Pressure3pm\",\n",
    "                        \"Temp3pm\",\n",
    "                        \"Temp9am\",\n",
    "                         \"RainToday_Sc\",\n",
    "                         \"cast (Month as String) Month\",\n",
    "                         \"RainTomorrow_Sc\"\n",
    "                      \n",
    "                                                  )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and forming a particular location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Building a model to predict following day rainfal for one location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocData: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LocData=df1.filter($\"Location\"===\"Sydney\")toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "// to see the no of records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Long = 3337\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loc: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Loc =LocData.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "// checking the no of records after dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: Long = 3337\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the label column(Rain tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = (Loc.select(Loc(\"RainTomorrow_Sc\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"Raintoday_Sc\",\n",
    "          $\"Humidity3pm\",\n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Month  which is a categorical variable need  to be indexed and encoded to be used in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_72fd102314eb\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the MonIn( column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_8d6af8db4055\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "//setting the assembler with required fields for the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e9f6f4fb1c28\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val assembler=(new VectorAssembler().setInputCols(Array(\n",
    "                                                        \"Raintoday_Sc\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test)=df2.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Maximum iteration was set to 20,more iterations giving minor improvements in the accuracy but more runtime and docker failures\n",
    "// as a trade off maximum iteration was set to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_5e9bb89cde84\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr=new LogisticRegression().setMaxIter(20)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_09121463f2d1\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_5e9bb89cde84-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_5e9bb89cde84-regParam: 0.01\n",
       "})\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.1, 0.01))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_1112439e892a\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_1112439e892a\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming test data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[5876] at rdd at <console>:38\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions=results.select($\"prediction\",$\"label\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@f4808f\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new MulticlassMetrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720.0  37.0   \n",
      "125.0  133.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) no  pred no  rain\n",
    "(2,2) yes pred yes rain\n",
    "(1,2) no pred yes rain \n",
    "(2,1)  yes pred no rain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Double = 0.8403940886699507\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.write.overwrite().save(\"/home/model_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write.overwrite().save(\"/home/pipeline_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLoaded: org.apache.spark.ml.tuning.CrossValidatorModel = cv_1112439e892a\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelLoaded=CrossValidatorModel.load(\"/home/model_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "// loaded model is applied on dataframe to see the prediction vs label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfu: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfu= modelLoaded.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+-------+-----+------------+-----------+-----------+-------+-------+\n",
      "|label|Location|               Date|MaxTemp|Month|Raintoday_Sc|Humidity3pm|Pressure3pm|Temp9am|Temp3pm|\n",
      "+-----+--------+-------------------+-------+-----+------------+-----------+-----------+-------+-------+\n",
      "|    0|  Sydney|2009-01-01 00:00:00|   34.7|    1|           0|       22.0|     1000.7|   22.0|   34.3|\n",
      "|    0|  Sydney|2009-01-02 00:00:00|   22.7|    1|           0|       57.0|     1013.8|   20.1|   20.8|\n",
      "|    0|  Sydney|2009-01-03 00:00:00|   23.0|    1|           0|       49.0|     1019.4|   18.8|   21.4|\n",
      "|    0|  Sydney|2009-01-04 00:00:00|   24.6|    1|           0|       58.0|     1015.2|   23.0|   24.1|\n",
      "|    0|  Sydney|2009-01-05 00:00:00|   27.9|    1|           0|       60.0|     1008.9|   23.4|   26.2|\n",
      "|    0|  Sydney|2009-01-06 00:00:00|   28.3|    1|           0|       64.0|     1008.0|   23.1|   25.8|\n",
      "|    0|  Sydney|2009-01-07 00:00:00|   33.1|    1|           0|       67.0|     1005.1|   23.7|   26.1|\n",
      "|    0|  Sydney|2009-01-08 00:00:00|   22.4|    1|           0|       72.0|     1013.4|   21.2|   20.1|\n",
      "|    0|  Sydney|2009-01-09 00:00:00|   22.6|    1|           0|       47.0|     1018.8|   19.2|   21.2|\n",
      "|    0|  Sydney|2009-01-10 00:00:00|   23.4|    1|           0|       53.0|     1013.9|   19.7|   22.5|\n",
      "|    1|  Sydney|2009-01-11 00:00:00|   24.6|    1|           0|       57.0|     1011.5|   23.4|   23.9|\n",
      "|    0|  Sydney|2009-01-12 00:00:00|   26.8|    1|           1|       61.0|     1016.4|   23.2|   25.6|\n",
      "|    0|  Sydney|2009-01-13 00:00:00|   26.2|    1|           0|       56.0|     1016.5|   23.5|   25.4|\n",
      "|    0|  Sydney|2009-01-14 00:00:00|   29.7|    1|           0|       49.0|     1011.2|   25.5|   29.1|\n",
      "|    1|  Sydney|2009-01-15 00:00:00|   33.4|    1|           0|       39.0|     1006.0|   28.6|   33.1|\n",
      "|    0|  Sydney|2009-01-16 00:00:00|   25.0|    1|           1|       61.0|     1008.8|   21.4|   24.0|\n",
      "|    0|  Sydney|2009-01-17 00:00:00|   22.6|    1|           0|       48.0|     1020.5|   19.0|   21.6|\n",
      "|    0|  Sydney|2009-01-18 00:00:00|   23.3|    1|           0|       46.0|     1020.0|   19.5|   22.9|\n",
      "|    0|  Sydney|2009-01-19 00:00:00|   25.1|    1|           0|       57.0|     1015.0|   22.5|   23.8|\n",
      "|    1|  Sydney|2009-01-20 00:00:00|   28.7|    1|           0|       54.0|     1010.6|   24.7|   27.0|\n",
      "+-----+--------+-------------------+-------+-----+------------+-----------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show() //Trying to check the actual Vs prediction for the sydney for last 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testev: org.apache.spark.sql.DataFrame = [Location: string, Date: timestamp ... 4 more fields]\n"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testev=dfu.select(\"Location\",\"Date\",\"RawPrediction\",\"probability\",\"label\",\"prediction\").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|Location|               Date|       RawPrediction|         probability|label|prediction|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|  Sydney|2009-01-01 00:00:00|[4.57331538904891...|[0.98978181333271...|    0|       0.0|\n",
      "|  Sydney|2009-01-02 00:00:00|[1.29746752156461...|[0.78540846255106...|    0|       0.0|\n",
      "|  Sydney|2009-01-03 00:00:00|[1.87603926645339...|[0.86715552568127...|    0|       0.0|\n",
      "|  Sydney|2009-01-04 00:00:00|[1.46081926110891...|[0.81165794665687...|    0|       0.0|\n",
      "|  Sydney|2009-01-05 00:00:00|[1.50137464719798...|[0.81777941044916...|    0|       0.0|\n",
      "|  Sydney|2009-01-06 00:00:00|[1.22247032067356...|[0.77249798777921...|    0|       0.0|\n",
      "|  Sydney|2009-01-07 00:00:00|[1.13528386767648...|[0.75681269949008...|    0|       0.0|\n",
      "|  Sydney|2009-01-08 00:00:00|[0.26646608225673...|[0.56622512846416...|    0|       0.0|\n",
      "|  Sydney|2009-01-09 00:00:00|[1.98624049706552...|[0.87934483218758...|    0|       0.0|\n",
      "|  Sydney|2009-01-10 00:00:00|[1.67061483855565...|[0.84165777788009...|    0|       0.0|\n",
      "|  Sydney|2009-01-11 00:00:00|[1.50622444261359...|[0.81850099526160...|    1|       0.0|\n",
      "|  Sydney|2009-01-12 00:00:00|[0.55595513233343...|[0.63551612038846...|    0|       0.0|\n",
      "|  Sydney|2009-01-13 00:00:00|[1.70177030295239...|[0.84576580524958...|    0|       0.0|\n",
      "|  Sydney|2009-01-14 00:00:00|[2.42972333498789...|[0.91906595579628...|    0|       0.0|\n",
      "|  Sydney|2009-01-15 00:00:00|[3.37585787048006...|[0.96694145500993...|    1|       0.0|\n",
      "|  Sydney|2009-01-16 00:00:00|[0.40995411227710...|[0.60107687578220...|    0|       0.0|\n",
      "|  Sydney|2009-01-17 00:00:00|[1.94728306666785...|[0.87515008556962...|    0|       0.0|\n",
      "|  Sydney|2009-01-18 00:00:00|[2.16518014649208...|[0.89707880801366...|    0|       0.0|\n",
      "|  Sydney|2009-01-19 00:00:00|[1.51943165527458...|[0.82045477369610...|    0|       0.0|\n",
      "|  Sydney|2009-01-20 00:00:00|[1.96095360732707...|[0.87663611756848...|    1|       0.0|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// future ideas to expand\n",
    "// creating models to different climate zones\n",
    "// trying to predict next couple of days rainfall( e.g. 3 days) with the available data\n",
    "// Trying deep learning neural networks for better models\n",
    "// Estimate the amount of rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
