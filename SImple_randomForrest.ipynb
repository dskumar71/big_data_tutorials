{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.{SparkContext, SparkConf}\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.Imputer\n",
       "import org.apache.spark.sql._\n",
       "import SQLContext._\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.util.hashing.MurmurHash3\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mll..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set log level to ERROR (less verbose)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "// Import libs\n",
    "\n",
    "import  org.apache.spark._ \n",
    "import  spark.implicits._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql._\n",
    "import SQLContext._\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.util.hashing.MurmurHash3\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)  \n",
    "import java.io.{File, PrintWriter}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_RFI_df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_RFI_df = spark.\n",
    "        read.\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"mode\",\"DROPMALFORMED\").\n",
    "        option(\"delimiter\", \",\").\n",
    "        csv(\"weatherAUS-clean-FINAL.csv\").\n",
    "        na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 78309\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_RFI_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- RainToday_Sc: integer (nullable = true)\n",
      " |-- RainTomorrow_Sc: integer (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Pressure3pm_Sc: double (nullable = true)\n",
      " |-- Pressure9am_Sc: double (nullable = true)\n",
      " |-- date_Sc: timestamp (nullable = true)\n",
      " |-- date-day_Sc: integer (nullable = true)\n",
      " |-- date-month_Sc: integer (nullable = true)\n",
      " |-- date-year_Sc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_RFI_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//// explnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_RFI_df_s: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_RFI_df_s = weather_RFI_df.selectExpr(\"Date\",\"Location\",\n",
    "                       \n",
    "                        \"MaxTemp\",\n",
    "                       \n",
    "                          \"Humidity3pm\", \n",
    "                        \n",
    "                         \"Pressure3pm\",\n",
    "                         \n",
    "                        \"Temp3pm\",\n",
    "                        \"Temp9am\",\n",
    "                         \"RainToday_Sc\",\n",
    "                         \"cast (Month as String) Month\",\n",
    "                         \"RainTomorrow_Sc\"\n",
    "                      \n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation about the approach on why we are considering only location based data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_SYD_df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_SYD_df=weather_RFI_df_s.filter($\"Location\"===\"Sydney\")toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Long = 3337\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_RFI_df.count()\n",
    "weather_SYD_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the label column(Rain tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_SYD_df_lb: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_SYD_df_lb = (weather_SYD_df.select(weather_SYD_df(\"RainTomorrow_Sc\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "        \n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"RainToday_Sc\",\n",
    "        \n",
    "          $\"Humidity3pm\",\n",
    "          \n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_982802b57382\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the MonIn( column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_dcc4d15f8c0e\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_d455f33b9cf3\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler=(new VectorAssembler().setInputCols(Array(\n",
    "                                                        \"RainToday_Sc\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                      \n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test)=weather_SYD_df_lb.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_7a1b9d928105\n",
       "res15: org.apache.spark.ml.classification.RandomForestClassifier = rfc_7a1b9d928105\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val rf = new RandomForestClassifier()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setNumTrees(10)\n",
    "      .setMaxBins(41)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_pipeline: org.apache.spark.ml.Pipeline = pipeline_1951312b7192\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val M_pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trg_model: org.apache.spark.ml.PipelineModel = pipeline_1951312b7192\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Trg_model=M_pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trg_model.write\n",
    "  .overwrite()\n",
    "  .save(\"spark-RandomForrest-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegModel: org.apache.spark.ml.PipelineModel = pipeline_1951312b7192\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// And load it back in during production\n",
    "val RegModel = PipelineModel.load(\"spark-RandomForrest-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =RegModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_6c9a1c17129d\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy: Double = 0.8394088669950739\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_RFI_df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_full_df_lb: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_full_df_lb = (weather_RFI_df_s.select(weather_SYD_df(\"RainTomorrow_Sc\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "        \n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"RainToday_Sc\",\n",
    "        \n",
    "          $\"Humidity3pm\",\n",
    "          \n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_a2d4760f7912\n",
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_330c31f00b09\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5d8cedddb62e\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")\n",
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))\n",
    "val assembler=(new VectorAssembler().setInputCols(Array(\n",
    "                                                        \"RainToday_Sc\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                      \n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Long = 78309\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_full_df_lb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_full: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test_full: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split the data\n",
    "val Array(training_full,test_full)=weather_full_df_lb.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf_full: org.apache.spark.ml.classification.RandomForestClassifier = rfc_0cc9ca9f683a\n",
       "res24: org.apache.spark.ml.classification.RandomForestClassifier = rfc_0cc9ca9f683a\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf_full = new RandomForestClassifier()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setNumTrees(10)\n",
    "      .setMaxBins(41)\n",
    "rf_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_pipeline_full: org.apache.spark.ml.Pipeline = pipeline_27876b697add\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the pipe line \n",
    "\n",
    "val M_pipeline_full =new Pipeline().setStages(Array(Indexer,Encoder,assembler,rf_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trg_model_full: org.apache.spark.ml.PipelineModel = pipeline_1951312b7192\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Trg_model_full=M_pipeline.fit(training_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trg_model_full.write\n",
    "  .overwrite()\n",
    "  .save(\"spark-RandomForrest-model-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegModel_full: org.apache.spark.ml.PipelineModel = pipeline_1951312b7192\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RegModel_full = PipelineModel.load(\"spark-RandomForrest-model-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results_full: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results_full =RegModel_full.transform(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator_full: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_dc2a33402004\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val evaluator_full = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy_full: Double = 0.8394088669950739\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy_full = evaluator_full.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
