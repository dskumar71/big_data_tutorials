{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://57b01b7ba74d:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591585066842)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 02:57:42,105 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "//Australia having very distinct 8 climate zones,no single model will work for all australia \n",
    "// Even within a state  there are multiple climate zones(from alpine to high humid) \n",
    "//Different models need to be trained for different locations\n",
    "//In this approach,separate models will be created for different locations and saved in the directory\n",
    "//for the prediction particular location model will be loaded and prediction will be done\n",
    "//Data used was weatherAUS_YN.csv(before imputing the  null values) as it was  observed  that \n",
    "//dropping null values are more effective than imputing (In our scenario imputing means blunting the features i.e.  generalising the data)\n",
    "//Wind direction (categorical columns) not tested or used in the modelling as too much of encoding required and crashing the docker\n",
    "//correlation matrix(wrangling file)& trial & Error methods were used to select the features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator, CrossValidatorModel}\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator,CrossValidatorModel}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@465cd5d4\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").\n",
    "    option(\"inferSchema\",\"true\").\n",
    "    format(\"csv\").\n",
    "    load(\"/home/weatherAUS_YN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// month is derived from the date filed to include the seasonal pattern to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df= data.withColumn(\"Month\",month(data(\"Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Selecting required fields with right data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "//based on correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = df.selectExpr(\"Date\",\"Location\",\n",
    "                        //\"cast(MinTemp as double) MinTemp\",\n",
    "                        \"cast (MaxTemp as double) MaxTemp\",\n",
    "                       // \"cast (Rainfall as double) Rainfall\",\n",
    "                       // \"cast (Evaporation as double) Evaporation\",\n",
    "                        // \"cast (Sunshine as double) Sunshine\",\n",
    "                        //\"WindGustDir\",\n",
    "                        // \"cast (WindGustSpeed as double) WindGustSpeed\",\n",
    "                        //\"WindDir9am\",\"WindDir3pm\",\n",
    "                         //\"cast (WindSpeed9am as double) WindSpeed9am\",\n",
    "                         //\"cast (WindSpeed3pm as double) WindSpeed3pm\",\n",
    "                         //\"cast (Humidity9am as double) Humidity9am\",\n",
    "                          \"cast (Humidity3pm as double) Humidity3pm\", \n",
    "                         //\"cast (Pressure9am as double) Pressure9am\",\n",
    "                         \"cast (Pressure3pm as double) Pressure3pm\",\n",
    "                         //\"cast (Cloud9am as double) Cloud9am\",\n",
    "                         //\"cast (Cloud3pm as double) Cloud3pm\",\n",
    "                        \"cast (Temp3pm as double) Temp3pm\",\n",
    "                        \"cast (Temp9am as double) Temp9am\",\n",
    "                         \"cast (RainToday as integer) RainToday\",\n",
    "                         \"cast (Month as String) Month\",\n",
    "                         \"cast (RainTomorrow as integer) RainTomorrow\"\n",
    "                      \n",
    "                                                  )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[String] = Array(Date, Location, MaxTemp, Humidity3pm, Pressure3pm, Temp3pm, Temp9am, RainToday, Month, RainTomorrow)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and forming a particular location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// different models need to be created for different locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocData: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LocData=df1.filter($\"Location\"===\"Perth\")toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// to see the no of records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 3193\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loc: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Loc =LocData.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// checking the no of records after dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Long = 3184\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the label column(Rain tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = (Loc.select(Loc(\"RainTomorrow\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "          //$\"MinTemp\",\n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"Raintoday\",\n",
    "         //$\"WindDir3pm\",\n",
    "         //$\"WindSpeed9am\",\n",
    "         // $\"WindSpeed3pm\", \n",
    "         //$\"Humidity9am\",\n",
    "          $\"Humidity3pm\",\n",
    "          //$\"Pressure9am\",\n",
    "          //$\"Cloud3pm\", \n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Month  which is a categorical variable need  to be indexed and encoded to be used in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_4128594a2cfc\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the MonIn( column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_1a50c20fe27d\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "//setting the assembler with required fields for the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ded2ea5a68d0\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val assembler=(new VectorAssembler().setInputCols(Array(//\"MinTemp\",\n",
    "                                                        \"Raintoday\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                        //\"Humidity9am\",\n",
    "                                                        //\"WindSpeed3pm\",\n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test)=df2.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Maximum iteration was set to 30,more iterations giving minor improvements in the accuracy but more runtime and docker failures\n",
    "//Threshold was set at 0.50 for the maximum accuracy(after trying multiple values)\n",
    "// Thereshold can be adjusted if other evaluation parameters are preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_2b24d4f81c5d\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr=new LogisticRegression().setMaxIter(10)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_6e0fb8f90b93\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_2b24d4f81c5d-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_2b24d4f81c5d-regParam: 0.01\n",
       "})\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.1, 0.01))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_3225ea819447\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 03:03:37,457 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-06-08 03:03:37,475 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2020-06-08 03:03:40,076 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:41,981 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:43,490 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:44,960 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:46,250 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:47,509 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:48,692 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:49,910 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:03:51,088 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:01,346 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:02,600 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:03,785 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:04,990 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:06,025 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:07,215 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:08,343 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:09,431 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "2020-06-08 03:04:10,626 ERROR [Thread-4] optimize.LBFGS (Logger.scala:error(27)) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_3225ea819447\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val model1=pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforming test data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[855] at rdd at <console>:38\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions=results.select($\"prediction\",$\"label\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@34048db0\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new MulticlassMetrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764.0  18.0  \n",
      "141.0  49.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) no  pred no  rain\n",
    "(2,2) yes pred yes rain\n",
    "(1,2) no pred yes rain \n",
    "(2,1)  yes pred no rain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Double = 0.8364197530864198\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.write.overwrite().save(\"/home/model_Perth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write.overwrite().save(\"/home/pipeline_Perth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLoaded: org.apache.spark.ml.tuning.CrossValidatorModel = cv_3225ea819447\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelLoaded=CrossValidatorModel.load(\"/home/model_Perth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "// loaded model is applied on dataframe to see the prediction vs label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfu: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfu= modelLoaded.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "|label|Location|               Date|MaxTemp|Month|Raintoday|Humidity3pm|Pressure3pm|Temp9am|Temp3pm|\n",
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "|    0|   Perth|2008-07-01 00:00:00|   18.8|    7|        0|       53.0|     1024.5|    8.5|   18.1|\n",
      "|    0|   Perth|2008-07-02 00:00:00|   20.7|    7|        0|       39.0|     1019.0|   11.1|   19.7|\n",
      "|    1|   Perth|2008-07-03 00:00:00|   19.9|    7|        0|       71.0|     1015.6|   12.1|   17.7|\n",
      "|    1|   Perth|2008-07-04 00:00:00|   19.2|    7|        1|       73.0|     1018.4|   13.2|   17.7|\n",
      "|    1|   Perth|2008-07-05 00:00:00|   16.4|    7|        1|       57.0|     1022.1|   15.9|   16.0|\n",
      "|    0|   Perth|2008-07-06 00:00:00|   15.9|    7|        1|       41.0|     1029.6|    6.9|   15.5|\n",
      "|    0|   Perth|2008-07-07 00:00:00|   18.3|    7|        0|       36.0|     1024.2|    8.7|   17.9|\n",
      "|    1|   Perth|2008-07-08 00:00:00|   20.4|    7|        0|       42.0|     1021.1|   10.2|   19.3|\n",
      "|    1|   Perth|2008-07-09 00:00:00|   19.5|    7|        1|       64.0|     1024.9|   12.1|   18.7|\n",
      "|    1|   Perth|2008-07-10 00:00:00|   20.4|    7|        1|       50.0|     1014.0|   13.4|   19.0|\n",
      "|    1|   Perth|2008-07-11 00:00:00|   17.1|    7|        1|       77.0|     1015.2|   15.4|   14.8|\n",
      "|    0|   Perth|2008-07-12 00:00:00|   17.3|    7|        1|       59.0|     1018.7|    9.6|   16.2|\n",
      "|    0|   Perth|2008-07-13 00:00:00|   18.8|    7|        0|       63.0|     1024.1|    9.5|   17.2|\n",
      "|    0|   Perth|2008-07-14 00:00:00|   19.7|    7|        0|       41.0|     1021.9|   11.7|   18.7|\n",
      "|    1|   Perth|2008-07-15 00:00:00|   17.1|    7|        0|       90.0|     1008.1|   15.8|   14.9|\n",
      "|    1|   Perth|2008-07-16 00:00:00|   17.1|    7|        1|       54.0|     1014.8|   13.2|   15.9|\n",
      "|    1|   Perth|2008-07-17 00:00:00|   16.9|    7|        1|       75.0|     1010.1|   10.5|   14.4|\n",
      "|    1|   Perth|2008-07-18 00:00:00|   17.9|    7|        1|       50.0|     1002.2|   16.8|   13.9|\n",
      "|    0|   Perth|2008-07-19 00:00:00|   14.3|    7|        1|       49.0|     1023.1|    8.0|   13.9|\n",
      "|    0|   Perth|2008-07-20 00:00:00|   17.0|    7|        0|       52.0|     1028.5|    7.5|   15.6|\n",
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testev: org.apache.spark.sql.DataFrame = [Location: string, Date: timestamp ... 4 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testev=dfu.select(\"Location\",\"Date\",\"RawPrediction\",\"probability\",\"label\",\"prediction\").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|Location|               Date|       RawPrediction|         probability|label|prediction|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|   Perth|2008-07-01 00:00:00|[0.73385840861598...|[0.67565140205561...|    0|       0.0|\n",
      "|   Perth|2008-07-02 00:00:00|[1.22838640216562...|[0.77353603213384...|    0|       0.0|\n",
      "|   Perth|2008-07-03 00:00:00|[0.32639785558474...|[0.58088266236470...|    1|       0.0|\n",
      "|   Perth|2008-07-04 00:00:00|[-0.4721342546714...|[0.38411121870937...|    1|       1.0|\n",
      "|   Perth|2008-07-05 00:00:00|[-0.2799448204272...|[0.43046730412009...|    1|       1.0|\n",
      "|   Perth|2008-07-06 00:00:00|[0.06049107625464...|[0.51511815935146...|    0|       0.0|\n",
      "|   Perth|2008-07-07 00:00:00|[1.11621338780469...|[0.75328566344203...|    0|       0.0|\n",
      "|   Perth|2008-07-08 00:00:00|[1.12358031850862...|[0.75465222469232...|    1|       0.0|\n",
      "|   Perth|2008-07-09 00:00:00|[-0.1967696333406...|[0.45096570001317...|    1|       1.0|\n",
      "|   Perth|2008-07-10 00:00:00|[0.19751120157436...|[0.54921790236173...|    1|       0.0|\n",
      "|   Perth|2008-07-11 00:00:00|[-0.7911581207181...|[0.31192005255131...|    1|       1.0|\n",
      "|   Perth|2008-07-12 00:00:00|[-0.2812776510967...|[0.43014057074531...|    0|       1.0|\n",
      "|   Perth|2008-07-13 00:00:00|[0.44852739312899...|[0.61028905147993...|    0|       0.0|\n",
      "|   Perth|2008-07-14 00:00:00|[1.09059221276730...|[0.74849322272809...|    0|       0.0|\n",
      "|   Perth|2008-07-15 00:00:00|[-0.3775061544003...|[0.40672852283249...|    1|       1.0|\n",
      "|   Perth|2008-07-16 00:00:00|[-0.1809185203173...|[0.45489333695784...|    1|       1.0|\n",
      "|   Perth|2008-07-17 00:00:00|[-0.7677803033476...|[0.31695946670730...|    1|       1.0|\n",
      "|   Perth|2008-07-18 00:00:00|[-0.1426564966167...|[0.46439623602464...|    1|       1.0|\n",
      "|   Perth|2008-07-19 00:00:00|[-0.2715834148384...|[0.43251841037363...|    0|       1.0|\n",
      "|   Perth|2008-07-20 00:00:00|[0.56418015079198...|[0.63741919373652...|    0|       0.0|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
