{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8c3c88d7baee:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590318209499)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.{SparkContext, SparkConf}\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.Imputer\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import required packages \n",
    "\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@68d2ac0d\n",
       "df: org.apache.spark.sql.DataFrame = [Date: string, Location: string ... 25 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read the csv file into a dataframe\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val df = sqlContext\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 25 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cast columns as appropriate types\n",
    "\n",
    "val df2 = df.withColumn(\"Date\",to_date($\"Date\",\"dd/MM/yyyy\"))\n",
    "            .withColumn(\"MinTemp\",col(\"MinTemp\").cast(DoubleType))\n",
    "            .withColumn(\"MaxTemp\",col(\"MaxTemp\").cast(DoubleType))\n",
    "            .withColumn(\"Rainfall\",col(\"Rainfall\").cast(DoubleType))\n",
    "            .withColumn(\"Evaporation\",col(\"Evaporation\").cast(DoubleType))\n",
    "            .withColumn(\"Sunshine\",col(\"Sunshine\").cast(DoubleType))\n",
    "            .withColumn(\"WindGustSpeed\",col(\"WindGustSpeed\").cast(DoubleType))\n",
    "            .withColumn(\"WindSpeed9am\",col(\"WindSpeed9am\").cast(DoubleType))\n",
    "            .withColumn(\"WindSpeed3pm\",col(\"WindSpeed3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Humidity9am\",col(\"Humidity9am\").cast(DoubleType))\n",
    "            .withColumn(\"Humidity3pm\",col(\"Humidity3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Pressure9am\",col(\"Pressure9am\").cast(DoubleType))\n",
    "            .withColumn(\"Pressure3pm\",col(\"Pressure3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Cloud9am\",col(\"Cloud9am\").cast(DoubleType))\n",
    "            .withColumn(\"Cloud3pm\",col(\"Cloud3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Temp9am\",col(\"Temp9am\").cast(DoubleType))\n",
    "            .withColumn(\"Temp3pm\",col(\"Temp3pm\").cast(DoubleType))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 25 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove any values from numeric columns which are outside expected range\n",
    "\n",
    "val df3 = df2.withColumn(\"MinTemp\",when(col(\"MinTemp\")<(-10),null).otherwise(col(\"MinTemp\")))\n",
    "            .withColumn(\"MinTemp\",when(col(\"MinTemp\")>(50),null).otherwise(col(\"MinTemp\")))\n",
    "            .withColumn(\"MaxTemp\",when(col(\"MaxTemp\")<(-10),null).otherwise(col(\"MaxTemp\")))\n",
    "            .withColumn(\"MaxTemp\",when(col(\"MaxTemp\")>(50),null).otherwise(col(\"MaxTemp\")))\n",
    "            .withColumn(\"Temp9am\",when(col(\"Temp9am\")<(-10),null).otherwise(col(\"Temp9am\")))\n",
    "            .withColumn(\"Temp9am\",when(col(\"Temp9am\")>(50),null).otherwise(col(\"Temp9am\")))\n",
    "            .withColumn(\"Temp3pm\",when(col(\"Temp3pm\")<(-10),null).otherwise(col(\"Temp3pm\")))\n",
    "            .withColumn(\"Temp3pm\",when(col(\"Temp3pm\")>(50),null).otherwise(col(\"Temp3pm\")))\n",
    "            .withColumn(\"Cloud9am\",when(col(\"Cloud9am\")<(0),null).otherwise(col(\"Cloud9am\")))\n",
    "            .withColumn(\"Cloud9am\",when(col(\"Cloud9am\")>(9),null).otherwise(col(\"Cloud9am\")))\n",
    "            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\")<(0),null).otherwise(col(\"Cloud3pm\")))\n",
    "            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\")>(9),null).otherwise(col(\"Cloud3pm\")))\n",
    "            .withColumn(\"Humidity9am\",when(col(\"Humidity9am\")<(0),null).otherwise(col(\"Humidity9am\")))\n",
    "            .withColumn(\"Humidity9am\",when(col(\"Humidity9am\")>(100),null).otherwise(col(\"Humidity9am\")))\n",
    "            .withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\")<(0),null).otherwise(col(\"Humidity3pm\")))\n",
    "            .withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\")>(100),null).otherwise(col(\"Humidity3pm\")))\n",
    "            .withColumn(\"Pressure9am\",when(col(\"Pressure9am\")<(970),null).otherwise(col(\"Pressure9am\")))\n",
    "            .withColumn(\"Pressure9am\",when(col(\"Pressure9am\")>(1050),null).otherwise(col(\"Pressure9am\")))\n",
    "            .withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\")<(970),null).otherwise(col(\"Pressure3pm\")))\n",
    "            .withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\")>(1050),null).otherwise(col(\"Pressure3pm\")))\n",
    "            .withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\")<(0),null).otherwise(col(\"WindGustSpeed\")))\n",
    "            .withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\")>(150),null).otherwise(col(\"WindGustSpeed\")))\n",
    "            .withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\")<(0),null).otherwise(col(\"WindSpeed9am\")))\n",
    "            .withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\")>(150),null).otherwise(col(\"WindSpeed9am\")))\n",
    "            .withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\")<(0),null).otherwise(col(\"WindSpeed3pm\")))\n",
    "            .withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\")>(150),null).otherwise(col(\"WindSpeed3pm\")))\n",
    "            .withColumn(\"Rainfall\",when(col(\"Rainfall\")<(0),null).otherwise(col(\"Rainfall\")))\n",
    "            .withColumn(\"Rainfall\",when(col(\"Rainfall\")>(500),null).otherwise(col(\"Rainfall\")))\n",
    "            .withColumn(\"Evaporation\",when(col(\"Evaporation\")<(0),null).otherwise(col(\"Evaporation\")))\n",
    "            .withColumn(\"Evaporation\",when(col(\"Evaporation\")>(100),null).otherwise(col(\"Evaporation\")))\n",
    "            .withColumn(\"Sunshine\",when(col(\"Sunshine\")<(0),null).otherwise(col(\"Sunshine\")))\n",
    "            .withColumn(\"Sunshine\",when(col(\"Sunshine\")>(15),null).otherwise(col(\"Sunshine\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract the month from the \"Date\" column and insert it as a new column\n",
    "\n",
    "val df4 = df3.withColumn(\"Month\",month(col(\"Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 11:03:47,493 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "Rows dropped = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Date: date, Location: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove duplicates based on Date and Location\n",
    "\n",
    "val df5 = df4.dropDuplicates(\"Date\",\"Location\")\n",
    "println(\"Rows dropped = \"+(df3.count()-df4.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumCols: Array[String] = Array(MinTemp, MaxTemp, Temp9am, Temp3pm, Pressure9am, Pressure3pm, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Rainfall, Evaporation, Sunshine, Cloud9am, Cloud3pm, Humidity9am, Humidity3pm)\n",
       "imputer: org.apache.spark.ml.feature.Imputer = imputer_0bf742de7dd1\n",
       "df6: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Replace null values with average for each numeric column\n",
    "\n",
    "val NumCols = Array(\"MinTemp\",\"MaxTemp\",\"Temp9am\",\"Temp3pm\",\"Pressure9am\",\"Pressure3pm\",\"WindGustSpeed\",\"WindSpeed9am\",\n",
    "                    \"WindSpeed3pm\",\"Rainfall\",\"Evaporation\",\"Sunshine\",\"Cloud9am\",\"Cloud3pm\",\"Humidity9am\",\"Humidity3pm\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(NumCols)\n",
    "  .setOutputCols(NumCols)\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "val df6 = imputer.fit(df5).transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_array: Array[Any] = Array(Cairns, NorfolkIsland, Bendigo, Walpole, Canberra, Woomera, Adelaide, Cobar, SydneyAirport, PerthAirport, Wollongong, Williamtown, Moree, Mildura, Portland, Albany, SalmonGums, Brisbane, Sydney, Perth, Sale, BadgerysCreek, Hobart, Tuggeranong, Ballarat, GoldCoast, MelbourneAirport, Dartmoor, Nhil, PearceRAAF, Albury, Witchcliffe, WaggaWagga, Darwin, Uluru, Nuriootpa, CoffsHarbour, Melbourne, Penrith, MountGambier, NorahHead, Katherine, MountGinini, Townsville, Newcastle, AliceSprings, Watsonia, Richmond, Launceston)\n",
       "location_list: List[Any] = List(Cairns, NorfolkIsland, Bendigo, Walpole, Canberra, Woomera, Adelaide, Cobar, SydneyAirport, PerthAirport, Wollongong, Williamtown, Moree, Mildura, Portland, Albany, SalmonGums, Brisbane, Sydney, Perth, Sale, ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a list of unique locations, for potential use to improve imputations\n",
    "\n",
    "val location_array = df5.select(\"Location\").distinct().rdd.map(r => r(0)).collect()\n",
    "val location_list = location_array.toList\n",
    "location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This section of code does not work, I was trying to split the dataframe up by location in order to improve the imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val byLocArray = location_array.map(location => df5.where($\"Location\" <=> location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val dfArray = df5.where(df5.col(\"Location\") == x) for (x <- location_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// for (x <- location_list){\n",
    "//    val dfArray = (df5.filter(df5(\"Location\") === x))\n",
    "//}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various ways to display, filter and aggregate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|       Pressure3pm|count|\n",
      "+------------------+-----+\n",
      "|1015.2582035378904|13981|\n",
      "|            1015.5|  773|\n",
      "|            1015.3|  767|\n",
      "|            1015.7|  763|\n",
      "|            1015.6|  761|\n",
      "|            1015.1|  752|\n",
      "|            1015.8|  751|\n",
      "|            1013.5|  751|\n",
      "|            1015.4|  745|\n",
      "|            1016.0|  738|\n",
      "|            1014.8|  735|\n",
      "|            1015.2|  730|\n",
      "|            1014.9|  727|\n",
      "|            1013.4|  723|\n",
      "|            1016.5|  719|\n",
      "|            1013.3|  717|\n",
      "|            1013.7|  717|\n",
      "|            1014.2|  716|\n",
      "|            1017.1|  715|\n",
      "|            1013.9|  711|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.groupBy(\"Pressure3pm\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|WindGustDir|count|\n",
      "+-----------+-----+\n",
      "|        SSE| 8993|\n",
      "|         NW| 8003|\n",
      "|         SW| 8797|\n",
      "|         NA| 9330|\n",
      "|          E| 9071|\n",
      "|        WSW| 8901|\n",
      "|        ENE| 7992|\n",
      "|         NE| 7060|\n",
      "|        NNW| 6561|\n",
      "|          N| 9033|\n",
      "|        SSW| 8610|\n",
      "|          W| 9780|\n",
      "|          S| 8949|\n",
      "|         SE| 9309|\n",
      "|        WNW| 8066|\n",
      "|        NNE| 6433|\n",
      "|        ESE| 7305|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.groupBy(\"WindGustDir\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------+-----------------+-----------------+\n",
      "|      Date|MinTemp|MaxTemp|Rainfall|      Evaporation|         Sunshine|\n",
      "+----------+-------+-------+--------+-----------------+-----------------+\n",
      "|2010-07-26|    1.9|   14.9|     0.2|5.468109011788718|7.624853113193593|\n",
      "|2011-05-02|   12.3|   22.3|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2011-05-28|    2.6|   13.9|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2011-10-24|   18.0|   28.4|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2011-12-22|   16.4|   29.6|     0.6|5.468109011788718|7.624853113193593|\n",
      "|2013-07-28|    3.0|   16.1|     0.2|5.468109011788718|7.624853113193593|\n",
      "|2013-09-03|    8.0|   23.3|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2014-02-09|   18.8|   42.0|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2014-07-11|    7.1|   11.3|     0.4|5.468109011788718|7.624853113193593|\n",
      "|2015-02-04|   15.2|   29.5|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2015-08-04|   -2.4|    8.6|     1.4|5.468109011788718|7.624853113193593|\n",
      "|2015-09-24|    1.7|   16.9|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2016-04-20|    8.8|   27.0|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2009-01-09|   12.5|   28.4|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2011-07-25|    5.5|   13.2|    16.2|5.468109011788718|7.624853113193593|\n",
      "|2011-09-15|    1.7|   17.2|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2012-02-14|   14.7|   31.4|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2014-06-16|    1.7|   11.7|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2014-06-18|    4.2|   11.3|     0.0|5.468109011788718|7.624853113193593|\n",
      "|2014-11-03|    5.8|   24.4|     0.0|5.468109011788718|7.624853113193593|\n",
      "+----------+-------+-------+--------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.filter(\"Location == 'Albury'\").select(\"Date\",\"MinTemp\",\"MaxTemp\",\"Rainfall\",\"Evaporation\",\"Sunshine\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|summary|           MinTemp|          MaxTemp|           Temp9am|           Temp3pm|       Pressure9am|       Pressure3pm|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|  count|            142193|           142193|            142193|            142193|            142193|            142193|\n",
      "|   mean|12.186399728729262|23.22678419127236|16.987508581701345|21.687234973147774|1017.6537584159646|1015.2582035378907|\n",
      "| stddev|6.3889236757881696|7.109554494595336| 6.472165978773238| 6.870770835439069| 6.746248325436812| 6.681787586654765|\n",
      "|    min|              -8.5|             -4.8|              -7.2|              -5.4|             980.5|             977.1|\n",
      "|    max|              33.9|             48.1|              40.2|              46.7|            1041.0|            1039.6|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|     WindGustSpeed|      WindSpeed9am|      WindSpeed3pm|          Rainfall|       Evaporation|         Sunshine|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            142193|            142193|            142193|            142193|            142193|           142193|\n",
      "|   mean| 39.98429165757611|14.001988000994022|18.637575861797195|2.3499740743108415| 5.468109011788734|7.624853113193581|\n",
      "| stddev|13.138384980787293| 8.851081674635614|  8.72155119991209| 8.423216965695465|3.1464309871076233|2.734927403397183|\n",
      "|    min|               6.0|               0.0|               0.0|               0.0|               0.0|              0.0|\n",
      "|    max|             135.0|             130.0|              87.0|             371.0|              86.2|             14.5|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|          Cloud9am|          Cloud3pm|       Humidity9am|       Humidity3pm|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|            142193|            142193|            142193|            142193|\n",
      "|   mean| 4.437189391885758| 4.503166899728541| 68.84381031057065|51.482606091656265|\n",
      "| stddev|2.2780803282090547|2.1047086299707987|18.932076724055975|20.532065476037133|\n",
      "|    min|               0.0|               0.0|               0.0|               0.0|\n",
      "|    max|               9.0|               9.0|             100.0|             100.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(\"MinTemp\",\"MaxTemp\",\"Temp9am\",\"Temp3pm\",\"Pressure9am\",\"Pressure3pm\").describe().show()\n",
    "df6.select(\"WindGustSpeed\",\"WindSpeed9am\",\"WindSpeed3pm\",\"Rainfall\",\"Evaporation\",\"Sunshine\").describe().show()\n",
    "df6.select(\"Cloud9am\",\"Cloud3pm\",\"Humidity9am\",\"Humidity3pm\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|     Location|max(Rainfall)|\n",
      "+-------------+-------------+\n",
      "|       Cairns|        278.4|\n",
      "|NorfolkIsland|        156.8|\n",
      "|      Bendigo|         66.4|\n",
      "|      Walpole|         93.2|\n",
      "|     Canberra|         87.0|\n",
      "|      Woomera|         66.4|\n",
      "|     Adelaide|         75.2|\n",
      "|        Cobar|         68.0|\n",
      "|SydneyAirport|        106.8|\n",
      "| PerthAirport|         76.6|\n",
      "|   Wollongong|        192.0|\n",
      "|  Williamtown|        225.0|\n",
      "|        Moree|        113.0|\n",
      "|      Mildura|        155.0|\n",
      "|     Portland|         72.0|\n",
      "|       Albany|         49.4|\n",
      "|   SalmonGums|         55.4|\n",
      "|     Brisbane|        182.6|\n",
      "|       Sydney|        119.4|\n",
      "|        Perth|        114.4|\n",
      "+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.groupBy(\"Location\").agg(max(\"Rainfall\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
