{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Scala Program is used to model the pipe line for the Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Importing Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.{SparkContext, SparkConf}\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.Imputer\n",
       "import org.apache.spark.sql._\n",
       "import SQLContext._\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.util.hashing.MurmurHash3\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache...."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  org.apache.spark._ \n",
    "import  spark.implicits._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql._\n",
    "import SQLContext._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.util.hashing.MurmurHash3\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator  \n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import java.io.{File, PrintWriter}\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator,CrossValidatorModel}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Starting spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@187ff3f4\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Importing the Cleaned file - Output file from the Wrangling Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").\n",
    "    option(\"inferSchema\",\"true\").\n",
    "    format(\"csv\").\n",
    "    load(\"weatherAUS-clean-FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- RainToday_Sc: integer (nullable = true)\n",
      " |-- RainTomorrow_Sc: integer (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Pressure3pm_Sc: double (nullable = true)\n",
      " |-- Pressure9am_Sc: double (nullable = true)\n",
      " |-- date_Sc: timestamp (nullable = true)\n",
      " |-- date-day_Sc: integer (nullable = true)\n",
      " |-- date-month_Sc: integer (nullable = true)\n",
      " |-- date-year_Sc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4.0 Selecting required fields with right data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>  Based on  Feature selection done using the correlation matrix (separate Scala program attached ) following filelds were identified to be the most  influencing field for   the rainfall on the following day </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = data.selectExpr(\"Date\",\"Location\",\"MaxTemp\",\"Humidity3pm\", \"Pressure3pm\",\"Temp3pm\",\"Temp9am\",\"RainToday_Sc\",\n",
    "                         \"cast (Month as String) Month\",\"RainTomorrow_Sc\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 142193\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Filtering and forming a particular location data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Australia having very distinct 8 climate zones,no single model will work for all australia  Even within a state  there are multiple climate zones(from alpine to high humid) \n",
    "Different models need to be trained for different locations\n",
    "In this approach,separate models will be created for different locations and saved in the directory for the prediction particular location model will be loaded and prediction will be done\n",
    "Data used was weatherAUS_YN.csv(before imputing the  null values) as it was  observed  that \n",
    "dropping null values are more effective than imputing (In our scenario imputing means blunting the features i.e.  generalising the data)\n",
    "Wind direction (categorical columns) not tested or used in the modelling as too much of encoding required and crashing the docker\n",
    "correlation matrix(wrangling file)& trial & Error methods were used to select the features) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Dropping the null values if any before the modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loc: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Loc =df1.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 130412\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We have decided to impute NA with Nearest values ( previous day or next day) or monthly average. Even then for some of the weather stations the data for the whole month is empty . Hence we were not able to impute with suitable values.\n",
    "\n",
    "Hence our group decided that  we will remove NA rows  as it might impact drastically the model efficiency </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Modeling  - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The problem is to predict the rain for tomorrow based on Labeled data set . Hence it is Supervised Learning probelm. Moreover since it is to predict whether to rain or not , it is Binary classification. We have decided to model both logistic regression and Random forrest and adopt a model which is best suited for the dataset </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Setting the label column(Rain tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = (Loc.select(Loc(\"RainTomorrow_Sc\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"Raintoday_Sc\",\n",
    "          $\"Humidity3pm\",\n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Month  which is a categorical variable need  to be indexed and encoded to be used in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_6298c04178fd\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3  Encoding the MonIn( column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_681ca8d3e6bf\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4  Setting the assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "//setting the assembler with required fields for the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5447d5a4e4df\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val assembler=(new VectorAssembler().setInputCols(Array(\n",
    "                                                        \"Raintoday_Sc\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Splitting the data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test)=df2.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test_reg: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Test is generated again one for use with Logistic regression. second one for Random forrest\n",
    "val Array(training,test_reg)=df2.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0  Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Maximum iteration was set to 20,more iterations giving minor improvements in the accuracy but more runtime and docker failures\n",
    "// as a trade off maximum iteration was set to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create an instance of the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_f64000ebf7c1\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr=new LogisticRegression().setMaxIter(20)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2  Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_bc44668ec9bb\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3  Cross validation hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_f64000ebf7c1-regParam: 0.1\n",
       "}, {\n",
       "\tlogreg_f64000ebf7c1-regParam: 0.01\n",
       "})\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.1, 0.01))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_36db993559d5\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new BinaryClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 22:24:59,760 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-06-08 22:24:59,761 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_36db993559d5\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Transforming test data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5  Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[654] at rdd at <console>:71\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions=results.select($\"prediction\",$\"label\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5240c6cd\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new MulticlassMetrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29343.0  1053.0  \n",
      "6189.0   2706.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) no  pred no  rain\n",
    "(2,2) yes pred yes rain\n",
    "(1,2) no pred yes rain \n",
    "(2,1)  yes pred no rain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Double = 0.8156829808353058\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.write.overwrite().save(\"/home/model_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write.overwrite().save(\"/home/pipeline_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8  Reloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLoaded: org.apache.spark.ml.tuning.CrossValidatorModel = cv_36db993559d5\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelLoaded=CrossValidatorModel.load(\"/home/model_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9 Using the model to run on Test Prediction using Test date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "// loaded model is applied on dataframe to see the prediction vs label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfu: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfu= modelLoaded.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+-------+-----+------------+-----------+-----------+-------+-------+\n",
      "|label|Location|               Date|MaxTemp|Month|Raintoday_Sc|Humidity3pm|Pressure3pm|Temp9am|Temp3pm|\n",
      "+-----+--------+-------------------+-------+-----+------------+-----------+-----------+-------+-------+\n",
      "|    1|  Cairns|2009-01-01 00:00:00|   31.4|    1|           1|       70.0|     1005.6|   28.0|   30.1|\n",
      "|    0|  Cairns|2009-01-02 00:00:00|   31.6|    1|           1|       70.0|     1006.8|   30.1|   30.5|\n",
      "|    0|  Cairns|2009-01-03 00:00:00|   31.6|    1|           0|       70.0|     1007.0|   30.8|   30.2|\n",
      "|    0|  Cairns|2009-01-04 00:00:00|   32.7|    1|           0|       76.0|     1006.6|   27.8|   28.5|\n",
      "|    1|  Cairns|2009-01-05 00:00:00|   31.6|    1|           0|       87.0|     1005.7|   27.8|   27.0|\n",
      "|    0|  Cairns|2009-01-06 00:00:00|   32.4|    1|           1|       63.0|     1004.4|   28.1|   31.1|\n",
      "|    0|  Cairns|2009-01-07 00:00:00|   32.4|    1|           0|       68.0|     1004.2|   28.4|   30.3|\n",
      "|    0|  Cairns|2009-01-08 00:00:00|   31.4|    1|           0|       67.0|     1003.2|   29.0|   30.3|\n",
      "|    1|  Cairns|2009-01-09 00:00:00|   29.6|    1|           0|       86.0|     1001.9|   24.9|   27.2|\n",
      "|    1|  Cairns|2009-01-10 00:00:00|   27.6|    1|           1|       93.0|     1003.0|   24.1|   24.8|\n",
      "|    1|  Cairns|2009-01-11 00:00:00|   28.7|    1|           1|       66.0|     1005.4|   25.5|   27.2|\n",
      "|    1|  Cairns|2009-01-12 00:00:00|   27.4|    1|           1|       98.0|      999.6|   24.7|   25.3|\n",
      "|    0|  Cairns|2009-01-13 00:00:00|   30.2|    1|           1|       67.0|     1001.2|   27.0|   28.7|\n",
      "|    0|  Cairns|2009-01-14 00:00:00|   30.9|    1|           0|       66.0|     1005.5|   28.9|   30.2|\n",
      "|    1|  Cairns|2009-01-15 00:00:00|   31.6|    1|           0|       69.0|     1008.7|   29.0|   30.0|\n",
      "|    1|  Cairns|2009-01-16 00:00:00|   31.4|    1|           1|       89.0|     1010.2|   29.8|   26.8|\n",
      "|    1|  Cairns|2009-01-17 00:00:00|   31.4|    1|           1|       68.0|     1008.2|   28.4|   30.5|\n",
      "|    1|  Cairns|2009-01-18 00:00:00|   32.2|    1|           1|       58.0|     1008.8|   27.6|   31.5|\n",
      "|    1|  Cairns|2009-01-19 00:00:00|   30.9|    1|           1|       62.0|     1008.6|   26.1|   30.7|\n",
      "|    1|  Cairns|2009-01-20 00:00:00|   29.2|    1|           1|       77.0|     1008.9|   24.4|   28.0|\n",
      "+-----+--------+-------------------+-------+-----+------------+-----------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show() //Trying to check the actual Vs prediction for the sydney for last 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testev: org.apache.spark.sql.DataFrame = [Location: string, Date: timestamp ... 4 more fields]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testev=dfu.select(\"Location\",\"Date\",\"RawPrediction\",\"probability\",\"label\",\"prediction\").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|Location|               Date|       RawPrediction|         probability|label|prediction|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|  Cairns|2009-01-01 00:00:00|[0.16930336203980...|[0.54222502845036...|    1|       0.0|\n",
      "|  Cairns|2009-01-02 00:00:00|[0.18251885673507...|[0.54550346235646...|    0|       0.0|\n",
      "|  Cairns|2009-01-03 00:00:00|[1.48166810914462...|[0.81482440653540...|    0|       0.0|\n",
      "|  Cairns|2009-01-04 00:00:00|[1.17925308399507...|[0.76481347970267...|    0|       0.0|\n",
      "|  Cairns|2009-01-05 00:00:00|[0.63547385908114...|[0.65372960486945...|    1|       0.0|\n",
      "|  Cairns|2009-01-06 00:00:00|[0.51394163530154...|[0.62573003026511...|    0|       0.0|\n",
      "|  Cairns|2009-01-07 00:00:00|[1.57360966927745...|[0.82829758646805...|    0|       0.0|\n",
      "|  Cairns|2009-01-08 00:00:00|[1.60898824555743...|[0.83327087023898...|    0|       0.0|\n",
      "|  Cairns|2009-01-09 00:00:00|[0.66025484245444...|[0.65931763295011...|    1|       0.0|\n",
      "|  Cairns|2009-01-10 00:00:00|[-1.0272173085596...|[0.26362394249204...|    1|       1.0|\n",
      "|  Cairns|2009-01-11 00:00:00|[0.26849068822848...|[0.56672233367074...|    1|       0.0|\n",
      "|  Cairns|2009-01-12 00:00:00|[-1.2546522498325...|[0.22189585114561...|    1|       1.0|\n",
      "|  Cairns|2009-01-13 00:00:00|[0.25538829688859...|[0.56350229701481...|    0|       0.0|\n",
      "|  Cairns|2009-01-14 00:00:00|[1.65463982160322...|[0.83951714996576...|    0|       0.0|\n",
      "|  Cairns|2009-01-15 00:00:00|[1.52738247815694...|[0.82162301668488...|    1|       0.0|\n",
      "|  Cairns|2009-01-16 00:00:00|[-0.7546772723324...|[0.31980300185866...|    1|       1.0|\n",
      "|  Cairns|2009-01-17 00:00:00|[0.27576579866634...|[0.56850784884937...|    1|       0.0|\n",
      "|  Cairns|2009-01-18 00:00:00|[0.76024400560439...|[0.68140670770224...|    1|       0.0|\n",
      "|  Cairns|2009-01-19 00:00:00|[0.55059516413679...|[0.63427366261280...|    1|       0.0|\n",
      "|  Cairns|2009-01-20 00:00:00|[-0.2024473006852...|[0.44956032937897...|    1|       1.0|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testev.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Random Forrest Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data set prepared in step 5   will be reused for Random Forrest as well </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7.1 Create Instance of Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_bcce2132f425\n",
       "res16: org.apache.spark.ml.classification.RandomForestClassifier = rfc_bcce2132f425\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setNumTrees(10)\n",
    "      .setMaxBins(41)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M_pipeline: org.apache.spark.ml.Pipeline = pipeline_d175986af826\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val M_pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trg_model: org.apache.spark.ml.PipelineModel = pipeline_d175986af826\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Trg_model=M_pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Save the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trg_model.write\n",
    "  .overwrite()\n",
    "  .save(\"spark-RandomForrest-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Load the model to try with test data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegModel: org.apache.spark.ml.PipelineModel = pipeline_d175986af826\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RegModel = PipelineModel.load(\"spark-RandomForrest-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Run Prediction with the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =RegModel.transform(test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_7daedef83524\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Check the Accuracy of the Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy: Double = 0.8295029396044896\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// future ideas to expand\n",
    "// creating models to different climate zones\n",
    "// trying to predict next couple of days rainfall( e.g. 3 days) with the available data\n",
    "// Trying deep learning neural networks for better models\n",
    "// Estimate the amount of rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
