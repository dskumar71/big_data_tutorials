{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://6183b873be90:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591519064075)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Australia having very distinct climate zones,no single model will work for all australia\n",
    "//Different models need to be trained for different locations\n",
    "//In this approach,separate models will be created for different location and saved in the directory\n",
    "//for the prediction particular location model will be loaded and prediction will be done\n",
    "//Data used was weatherAUS_YN.csv(before imputing the  null values) as it was  observed  that \n",
    "//dropping null values are more effective than imputing (In our scenario imputing means blunting the features i.e.  generalising the data)\n",
    "//Wind direction (categorical columns) not tested or used in the modelling as too much of encoding required and crashing the docker\n",
    "//correlation matrix(wrangling file)& trial & Error methods were used to select the features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6bb3bb67\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").\n",
    "    option(\"inferSchema\",\"true\").\n",
    "    format(\"csv\").\n",
    "    load(\"/home/weatherAUS_YN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df= data.withColumn(\"Month\",month(data(\"Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Selecting required fields with right data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = df.selectExpr(\"Date\",\"Location\",\n",
    "                        //\"cast(MinTemp as double) MinTemp\",\n",
    "                        \"cast (MaxTemp as double) MaxTemp\",\n",
    "                       // \"cast (Rainfall as double) Rainfall\",\n",
    "                       // \"cast (Evaporation as double) Evaporation\",\n",
    "                        // \"cast (Sunshine as double) Sunshine\",\n",
    "                        //\"WindGustDir\",\n",
    "                        // \"cast (WindGustSpeed as double) WindGustSpeed\",\n",
    "                        //\"WindDir9am\",\"WindDir3pm\",\n",
    "                         //\"cast (WindSpeed9am as double) WindSpeed9am\",\n",
    "                         //\"cast (WindSpeed3pm as double) WindSpeed3pm\",\n",
    "                         //\"cast (Humidity9am as double) Humidity9am\",\n",
    "                          \"cast (Humidity3pm as double) Humidity3pm\", \n",
    "                         //\"cast (Pressure9am as double) Pressure9am\",\n",
    "                         \"cast (Pressure3pm as double) Pressure3pm\",\n",
    "                         //\"cast (Cloud9am as double) Cloud9am\",\n",
    "                         //\"cast (Cloud3pm as double) Cloud3pm\",\n",
    "                        \"cast (Temp3pm as double) Temp3pm\",\n",
    "                        \"cast (Temp9am as double) Temp9am\",\n",
    "                         \"cast (RainToday as integer) RainToday\",\n",
    "                         \"cast (Month as String) Month\",\n",
    "                         \"cast (RainTomorrow as integer) RainTomorrow\"\n",
    "                      \n",
    "                                                  )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(Date, Location, MaxTemp, Humidity3pm, Pressure3pm, Temp3pm, Temp9am, RainToday, Month, RainTomorrow)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and forming a particular location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocData: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LocData=df1.filter($\"Location\"===\"Sydney\")toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 3337\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loc: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Loc =LocData.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 3305\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the label column(Rain tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = (Loc.select(Loc(\"RainTomorrow\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "          //$\"MinTemp\",\n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"Raintoday\",\n",
    "         //$\"WindDir3pm\",\n",
    "         //$\"WindSpeed9am\",\n",
    "         // $\"WindSpeed3pm\", \n",
    "         //$\"Humidity9am\",\n",
    "          $\"Humidity3pm\",\n",
    "          //$\"Pressure9am\",\n",
    "          //$\"Cloud3pm\", \n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_4dc0a6d08cae\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the MonIn( column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_99b67ff2d6e2\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f6f327b2501d\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val assembler=(new VectorAssembler().setInputCols(Array(//\"MinTemp\",\n",
    "                                                        \"Raintoday\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                        //\"Humidity9am\",\n",
    "                                                        //\"WindSpeed3pm\",\n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test)=df2.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_0faf9be74669\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr=new LogisticRegression().setMaxIter(30).setThreshold(0.50)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_661796b4f06c\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1: org.apache.spark.ml.PipelineModel = pipeline_661796b4f06c\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model1=pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforming test data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =model1.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[438] at rdd at <console>:38\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions=results.select($\"prediction\",$\"label\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6abe84a1\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new MulticlassMetrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716.0  33.0   \n",
      "126.0  131.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) no  pred no  rain\n",
    "(2,2) yes pred yes rain\n",
    "(1,2) no pred yes rain \n",
    "(2,1)  yes pred no rain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Double = 0.841948310139165\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.write.overwrite().save(\"/home/model_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write.overwrite().save(\"/home/pipeline_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLoaded: org.apache.spark.ml.PipelineModel = pipeline_661796b4f06c\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelLoaded=PipelineModel.load(\"/home/model_Sydney\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfu: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfu= modelLoaded.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "|label|Location|               Date|MaxTemp|Month|Raintoday|Humidity3pm|Pressure3pm|Temp9am|Temp3pm|\n",
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "|    1|  Sydney|2008-02-01 00:00:00|   22.4|    2|        1|       84.0|     1017.4|   20.7|   20.9|\n",
      "|    1|  Sydney|2008-02-02 00:00:00|   25.6|    2|        1|       73.0|     1016.4|   22.4|   24.8|\n",
      "|    1|  Sydney|2008-02-03 00:00:00|   24.5|    2|        1|       86.0|     1015.6|   23.5|   23.0|\n",
      "|    1|  Sydney|2008-02-04 00:00:00|   22.8|    2|        1|       90.0|     1011.8|   21.4|   20.9|\n",
      "|    1|  Sydney|2008-02-05 00:00:00|   25.7|    2|        1|       74.0|     1004.8|   22.5|   25.5|\n",
      "|    1|  Sydney|2008-02-06 00:00:00|   27.2|    2|        1|       62.0|      998.6|   23.8|   26.0|\n",
      "|    1|  Sydney|2008-02-07 00:00:00|   26.3|    2|        1|       80.0|     1000.3|   21.7|   22.3|\n",
      "|    1|  Sydney|2008-02-08 00:00:00|   22.3|    2|        1|       61.0|     1007.4|   18.9|   21.1|\n",
      "|    1|  Sydney|2008-02-09 00:00:00|   20.8|    2|        1|       91.0|     1007.6|   17.1|   16.5|\n",
      "|    0|  Sydney|2008-02-10 00:00:00|   24.2|    2|        1|       53.0|     1013.4|   17.2|   23.3|\n",
      "|    0|  Sydney|2008-02-11 00:00:00|   23.9|    2|        0|       53.0|     1015.3|   18.9|   23.7|\n",
      "|    1|  Sydney|2008-02-12 00:00:00|   27.3|    2|        0|       67.0|     1007.5|   22.9|   24.7|\n",
      "|    1|  Sydney|2008-02-13 00:00:00|   22.8|    2|        1|       70.0|     1011.7|   20.9|   21.0|\n",
      "|    0|  Sydney|2008-02-14 00:00:00|   24.3|    2|        1|       51.0|     1016.5|   18.4|   23.3|\n",
      "|    0|  Sydney|2008-02-15 00:00:00|   24.4|    2|        0|       56.0|     1018.9|   19.5|   23.8|\n",
      "|    0|  Sydney|2008-02-16 00:00:00|   24.1|    2|        0|       52.0|     1022.6|   19.8|   23.3|\n",
      "|    0|  Sydney|2008-02-17 00:00:00|   24.8|    2|        0|       62.0|     1023.1|   21.8|   23.4|\n",
      "|    0|  Sydney|2008-02-18 00:00:00|   25.3|    2|        0|       59.0|     1019.4|   21.2|   23.8|\n",
      "|    0|  Sydney|2008-02-19 00:00:00|   24.8|    2|        0|       65.0|     1015.4|   20.8|   23.4|\n",
      "|    0|  Sydney|2008-02-20 00:00:00|   26.4|    2|        0|       60.0|     1010.8|   21.3|   25.2|\n",
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testev: org.apache.spark.sql.DataFrame = [Location: string, Date: timestamp ... 4 more fields]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testev=dfu.select(\"Location\",\"Date\",\"RawPrediction\",\"probability\",\"label\",\"prediction\").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|Location|               Date|       RawPrediction|         probability|label|prediction|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|  Sydney|2008-02-01 00:00:00|[-2.0041402762100...|[0.11876890447253...|    1|       1.0|\n",
      "|  Sydney|2008-02-02 00:00:00|[-0.8975070067006...|[0.28956307756246...|    1|       1.0|\n",
      "|  Sydney|2008-02-03 00:00:00|[-2.0135357891308...|[0.11778906017086...|    1|       1.0|\n",
      "|  Sydney|2008-02-04 00:00:00|[-2.4777528474604...|[0.07743257910936...|    1|       1.0|\n",
      "|  Sydney|2008-02-05 00:00:00|[-0.9609898951872...|[0.27668004514835...|    1|       1.0|\n",
      "|  Sydney|2008-02-06 00:00:00|[-0.0372598516185...|[0.49068611460667...|    1|       1.0|\n",
      "|  Sydney|2008-02-07 00:00:00|[-1.6607434764523...|[0.15966221932060...|    1|       1.0|\n",
      "|  Sydney|2008-02-08 00:00:00|[-0.2783735293004...|[0.43085257204365...|    1|       1.0|\n",
      "|  Sydney|2008-02-09 00:00:00|[-2.8803971770045...|[0.05313115157828...|    1|       1.0|\n",
      "|  Sydney|2008-02-10 00:00:00|[0.50321233175426...|[0.62321394359060...|    0|       0.0|\n",
      "|  Sydney|2008-02-11 00:00:00|[1.42012435413513...|[0.80535791047340...|    0|       0.0|\n",
      "|  Sydney|2008-02-12 00:00:00|[0.40094411425401...|[0.59891447255767...|    1|       0.0|\n",
      "|  Sydney|2008-02-13 00:00:00|[-0.9545216669272...|[0.27797638804020...|    1|       1.0|\n",
      "|  Sydney|2008-02-14 00:00:00|[0.66481864605342...|[0.66034199830497...|    0|       0.0|\n",
      "|  Sydney|2008-02-15 00:00:00|[1.21095677957790...|[0.77046819638596...|    0|       0.0|\n",
      "|  Sydney|2008-02-16 00:00:00|[1.49077307766705...|[0.81619427895389...|    0|       0.0|\n",
      "|  Sydney|2008-02-17 00:00:00|[0.74054603385241...|[0.67711524726642...|    0|       0.0|\n",
      "|  Sydney|2008-02-18 00:00:00|[0.98405535089211...|[0.72791214507730...|    0|       0.0|\n",
      "|  Sydney|2008-02-19 00:00:00|[0.48803409003003...|[0.61964320496584...|    0|       0.0|\n",
      "|  Sydney|2008-02-20 00:00:00|[0.97934358627746...|[0.72697795009562...|    0|       0.0|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
