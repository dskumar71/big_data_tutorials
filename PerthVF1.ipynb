{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://6183b873be90:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591528110560)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 11:08:26,432 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "//Australia having very distinct climate zones,no single model will work for all australia\n",
    "//Different models need to be trained for different locations\n",
    "//In this approach,separate models will be created for different location and saved in the directory\n",
    "//for the prediction particular location model will be loaded and prediction will be done\n",
    "//Data used was weatherAUS_YN.csv(before imputing the  null values) as it was  observed  that \n",
    "//dropping null values are more effective than imputing (In our scenario imputing means blunting the features i.e.  generalising the data)\n",
    "//Wind direction (categorical columns) not tested or used in the modelling as too much of encoding required and crashing the docker\n",
    "//correlation matrix(wrangling file)& trial & Error methods were used to select the features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.LogisticRegression  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator,VectorIndexer}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@a8f6d51\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").\n",
    "    option(\"inferSchema\",\"true\").\n",
    "    format(\"csv\").\n",
    "    load(\"/home/weatherAUS_YN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df= data.withColumn(\"Month\",month(data(\"Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Selecting required fields with right data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = df.selectExpr(\"Date\",\"Location\",\n",
    "                        //\"cast(MinTemp as double) MinTemp\",\n",
    "                        \"cast (MaxTemp as double) MaxTemp\",\n",
    "                       // \"cast (Rainfall as double) Rainfall\",\n",
    "                       // \"cast (Evaporation as double) Evaporation\",\n",
    "                        // \"cast (Sunshine as double) Sunshine\",\n",
    "                        //\"WindGustDir\",\n",
    "                        // \"cast (WindGustSpeed as double) WindGustSpeed\",\n",
    "                        //\"WindDir9am\",\"WindDir3pm\",\n",
    "                         //\"cast (WindSpeed9am as double) WindSpeed9am\",\n",
    "                         //\"cast (WindSpeed3pm as double) WindSpeed3pm\",\n",
    "                         //\"cast (Humidity9am as double) Humidity9am\",\n",
    "                          \"cast (Humidity3pm as double) Humidity3pm\", \n",
    "                         //\"cast (Pressure9am as double) Pressure9am\",\n",
    "                         \"cast (Pressure3pm as double) Pressure3pm\",\n",
    "                         //\"cast (Cloud9am as double) Cloud9am\",\n",
    "                         //\"cast (Cloud3pm as double) Cloud3pm\",\n",
    "                        \"cast (Temp3pm as double) Temp3pm\",\n",
    "                        \"cast (Temp9am as double) Temp9am\",\n",
    "                         \"cast (RainToday as integer) RainToday\",\n",
    "                         \"cast (Month as String) Month\",\n",
    "                         \"cast (RainTomorrow as integer) RainTomorrow\"\n",
    "                      \n",
    "                                                  )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(Date, Location, MaxTemp, Humidity3pm, Pressure3pm, Temp3pm, Temp9am, RainToday, Month, RainTomorrow)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and forming a particular location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocData: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LocData=df1.filter($\"Location\"===\"Perth\")toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 3193\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loc: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Loc =LocData.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 3184\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the label column(Rain tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = (Loc.select(Loc(\"RainTomorrow\").as(\"label\"), \n",
    "           $\"Location\",\n",
    "           $\"Date\",\n",
    "          //$\"MinTemp\",\n",
    "          $\"MaxTemp\", \n",
    "         $\"Month\",\n",
    "         $\"Raintoday\",\n",
    "         //$\"WindDir3pm\",\n",
    "         //$\"WindSpeed9am\",\n",
    "         // $\"WindSpeed3pm\", \n",
    "         //$\"Humidity9am\",\n",
    "          $\"Humidity3pm\",\n",
    "          //$\"Pressure9am\",\n",
    "          //$\"Cloud3pm\", \n",
    "          $\"Pressure3pm\", \n",
    "           $\"Temp9am\", \n",
    "          $\"Temp3pm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the month column(categorical)-To capture the seasonal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_ae36a46113bc\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Indexer=new StringIndexer().setInputCol(\"Month\").setOutputCol(\"MonIn\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the MonIn( column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_f1befb92b0e8\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Encoder=new OneHotEncoderEstimator().setInputCols(Array(\"MonIn\")).setOutputCols(Array(\"MonEn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_cc24206c4f2e\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val assembler=(new VectorAssembler().setInputCols(Array(//\"MinTemp\",\n",
    "                                                        \"Raintoday\",\n",
    "                                                         \"MaxTemp\",\n",
    "                                                        \"Humidity3pm\",\n",
    "                                                         \"Temp3pm\",\n",
    "                                                         \"MonEn\",\n",
    "                                                        //\"Humidity9am\",\n",
    "                                                        //\"WindSpeed3pm\",\n",
    "                                                        \"Pressure3pm\"\n",
    "                                                        )).setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Location: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training,test)=df2.randomSplit(Array(0.7,0.3),seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_0ffe5fc0ad75\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr=new LogisticRegression().setMaxIter(30).setThreshold(0.50)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_8ae362c04e98\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline=new Pipeline().setStages(Array(Indexer,Encoder,assembler,lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 11:11:42,905 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-06-07 11:11:42,909 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model1: org.apache.spark.ml.PipelineModel = pipeline_8ae362c04e98\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model1=pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforming test data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results =model1.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[109] at rdd at <console>:38\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions=results.select($\"prediction\",$\"label\").as[(Double,Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@57c85456\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics= new MulticlassMetrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747.0  35.0  \n",
      "108.0  82.0  \n"
     ]
    }
   ],
   "source": [
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) no  pred no  rain\n",
    "(2,2) yes pred yes rain\n",
    "(1,2) no pred yes rain \n",
    "(2,1)  yes pred no rain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Double = 0.852880658436214\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.write.overwrite().save(\"/home/model_Perth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write.overwrite().save(\"/home/pipeline_Perth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLoaded: org.apache.spark.ml.PipelineModel = pipeline_8ae362c04e98\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelLoaded=PipelineModel.load(\"/home/model_Perth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfu: org.apache.spark.sql.DataFrame = [label: int, Location: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfu= modelLoaded.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "|label|Location|               Date|MaxTemp|Month|Raintoday|Humidity3pm|Pressure3pm|Temp9am|Temp3pm|\n",
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "|    0|   Perth|2008-07-01 00:00:00|   18.8|    7|        0|       53.0|     1024.5|    8.5|   18.1|\n",
      "|    0|   Perth|2008-07-02 00:00:00|   20.7|    7|        0|       39.0|     1019.0|   11.1|   19.7|\n",
      "|    1|   Perth|2008-07-03 00:00:00|   19.9|    7|        0|       71.0|     1015.6|   12.1|   17.7|\n",
      "|    1|   Perth|2008-07-04 00:00:00|   19.2|    7|        1|       73.0|     1018.4|   13.2|   17.7|\n",
      "|    1|   Perth|2008-07-05 00:00:00|   16.4|    7|        1|       57.0|     1022.1|   15.9|   16.0|\n",
      "|    0|   Perth|2008-07-06 00:00:00|   15.9|    7|        1|       41.0|     1029.6|    6.9|   15.5|\n",
      "|    0|   Perth|2008-07-07 00:00:00|   18.3|    7|        0|       36.0|     1024.2|    8.7|   17.9|\n",
      "|    1|   Perth|2008-07-08 00:00:00|   20.4|    7|        0|       42.0|     1021.1|   10.2|   19.3|\n",
      "|    1|   Perth|2008-07-09 00:00:00|   19.5|    7|        1|       64.0|     1024.9|   12.1|   18.7|\n",
      "|    1|   Perth|2008-07-10 00:00:00|   20.4|    7|        1|       50.0|     1014.0|   13.4|   19.0|\n",
      "|    1|   Perth|2008-07-11 00:00:00|   17.1|    7|        1|       77.0|     1015.2|   15.4|   14.8|\n",
      "|    0|   Perth|2008-07-12 00:00:00|   17.3|    7|        1|       59.0|     1018.7|    9.6|   16.2|\n",
      "|    0|   Perth|2008-07-13 00:00:00|   18.8|    7|        0|       63.0|     1024.1|    9.5|   17.2|\n",
      "|    0|   Perth|2008-07-14 00:00:00|   19.7|    7|        0|       41.0|     1021.9|   11.7|   18.7|\n",
      "|    1|   Perth|2008-07-15 00:00:00|   17.1|    7|        0|       90.0|     1008.1|   15.8|   14.9|\n",
      "|    1|   Perth|2008-07-16 00:00:00|   17.1|    7|        1|       54.0|     1014.8|   13.2|   15.9|\n",
      "|    1|   Perth|2008-07-17 00:00:00|   16.9|    7|        1|       75.0|     1010.1|   10.5|   14.4|\n",
      "|    1|   Perth|2008-07-18 00:00:00|   17.9|    7|        1|       50.0|     1002.2|   16.8|   13.9|\n",
      "|    0|   Perth|2008-07-19 00:00:00|   14.3|    7|        1|       49.0|     1023.1|    8.0|   13.9|\n",
      "|    0|   Perth|2008-07-20 00:00:00|   17.0|    7|        0|       52.0|     1028.5|    7.5|   15.6|\n",
      "+-----+--------+-------------------+-------+-----+---------+-----------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testev: org.apache.spark.sql.DataFrame = [Location: string, Date: timestamp ... 4 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testev=dfu.select(\"Location\",\"Date\",\"RawPrediction\",\"probability\",\"label\",\"prediction\").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|Location|               Date|       RawPrediction|         probability|label|prediction|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "|   Perth|2008-07-01 00:00:00|[0.82942927553653...|[0.69623423961535...|    0|       0.0|\n",
      "|   Perth|2008-07-02 00:00:00|[2.15246309691057...|[0.89589871976251...|    0|       0.0|\n",
      "|   Perth|2008-07-03 00:00:00|[-0.9608150338322...|[0.27671504120268...|    1|       1.0|\n",
      "|   Perth|2008-07-04 00:00:00|[-1.7214488860469...|[0.15168463210551...|    1|       1.0|\n",
      "|   Perth|2008-07-05 00:00:00|[-0.1847097658736...|[0.45395340114355...|    1|       1.0|\n",
      "|   Perth|2008-07-06 00:00:00|[1.37035487013528...|[0.79743748212742...|    0|       0.0|\n",
      "|   Perth|2008-07-07 00:00:00|[2.44907996692623...|[0.92049414453850...|    0|       0.0|\n",
      "|   Perth|2008-07-08 00:00:00|[1.87014233994295...|[0.86647474652492...|    1|       0.0|\n",
      "|   Perth|2008-07-09 00:00:00|[-0.8053037418856...|[0.30889214167522...|    1|       1.0|\n",
      "|   Perth|2008-07-10 00:00:00|[0.46670231292280...|[0.61460294169405...|    1|       0.0|\n",
      "|   Perth|2008-07-11 00:00:00|[-2.1731818052300...|[0.10218475527749...|    1|       1.0|\n",
      "|   Perth|2008-07-12 00:00:00|[-0.4012766596340...|[0.40100564737382...|    0|       1.0|\n",
      "|   Perth|2008-07-13 00:00:00|[-0.1526559334949...|[0.46190995824658...|    0|       1.0|\n",
      "|   Perth|2008-07-14 00:00:00|[1.96259388188935...|[0.87681339588646...|    0|       0.0|\n",
      "|   Perth|2008-07-15 00:00:00|[-2.8484999913328...|[0.05475890638371...|    1|       1.0|\n",
      "|   Perth|2008-07-16 00:00:00|[0.04907417052708...|[0.51226608105667...|    1|       0.0|\n",
      "|   Perth|2008-07-17 00:00:00|[-2.0177552602596...|[0.11735130131852...|    1|       1.0|\n",
      "|   Perth|2008-07-18 00:00:00|[0.29100019611555...|[0.57224097984879...|    1|       0.0|\n",
      "|   Perth|2008-07-19 00:00:00|[0.55247349635154...|[0.63470927043976...|    0|       0.0|\n",
      "|   Perth|2008-07-20 00:00:00|[0.89803163782133...|[0.71054483569514...|    0|       0.0|\n",
      "+--------+-------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
