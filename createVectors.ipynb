{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.tree.RandomForest\n",
       "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import scala.collection.JavaConverters._\n",
       "import scala.util.hashing.{MurmurHash3=>MH3}\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.row_number\n",
       "import org.apache.spark.sql.types.StringType\n",
       "import org.apache.spark.graphx._\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import scala.collection.JavaConverters._ \n",
    "import scala.util.hashing.{ MurmurHash3 => MH3 }\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.row_number\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = spark.\n",
    "        read.\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"mode\",\"DROPMALFORMED\").\n",
    "        option(\"delimiter\", \",\").\n",
    "        csv(\"weatherAUS-clean-FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- RainToday_Sc: integer (nullable = true)\n",
      " |-- RainTomorrow_Sc: integer (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Pressure3pm_Sc: double (nullable = true)\n",
      " |-- Pressure9am_Sc: double (nullable = true)\n",
      " |-- date_Sc: timestamp (nullable = true)\n",
      " |-- date-day_Sc: integer (nullable = true)\n",
      " |-- date-month_Sc: integer (nullable = true)\n",
      " |-- date-year_Sc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-06 12:44:28,741 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    }
   ],
   "source": [
    "df0.registerTempTable(\"ausRainData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.sql(\"select * from ausRainData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- RainToday_Sc: integer (nullable = true)\n",
      " |-- RainTomorrow_Sc: integer (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Pressure3pm_Sc: double (nullable = true)\n",
      " |-- Pressure9am_Sc: double (nullable = true)\n",
      " |-- date_Sc: timestamp (nullable = true)\n",
      " |-- date-day_Sc: integer (nullable = true)\n",
      " |-- date-month_Sc: integer (nullable = true)\n",
      " |-- date-year_Sc: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val location = df1.select($\"Location\")\n",
    "  .flatMap(x => Iterable(x(0).toString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hash: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = MapPartitionsRDD[475] at map at <console>:135\n"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hash: RDD[(VertexId, String)] = location\n",
    "  .rdd\n",
    "  .distinct()\n",
    "  .map(x => (MurmurHash3.stringHash(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|         _1|              _2|\n",
      "+-----------+----------------+\n",
      "| -403901479|    CoffsHarbour|\n",
      "| -424784333|         Woomera|\n",
      "| 1735662482|         Bendigo|\n",
      "| 1885063312|         Mildura|\n",
      "|-1931965857|        Watsonia|\n",
      "|-1185911642|           Moree|\n",
      "|-1501181004|         Penrith|\n",
      "| 1612109533|      Wollongong|\n",
      "|  737484909|        Richmond|\n",
      "| 2126364083|    PerthAirport|\n",
      "|  220478170|      SalmonGums|\n",
      "|  461690079|   NorfolkIsland|\n",
      "| 1374240089|       NorahHead|\n",
      "|  160510013|        Portland|\n",
      "| 1038357001|     Witchcliffe|\n",
      "|-1394747672|          Cairns|\n",
      "|-1407311822|      WaggaWagga|\n",
      "|-2085394632|MelbourneAirport|\n",
      "|-1067979720|       Newcastle|\n",
      "| -942212088|      PearceRAAF|\n",
      "|-1075208953|          Sydney|\n",
      "|-2035869294|       GoldCoast|\n",
      "|  903326363|        Brisbane|\n",
      "|-2012030282|          Hobart|\n",
      "| -213329084|      Launceston|\n",
      "|  840634082|         Walpole|\n",
      "|  288157406|    AliceSprings|\n",
      "| -583384466|        Canberra|\n",
      "| -365093340|        Dartmoor|\n",
      "|  337013719|    MountGambier|\n",
      "| -221632718|          Darwin|\n",
      "| 1382859019|     Williamtown|\n",
      "|-2029043205|        Adelaide|\n",
      "| 1099193263|           Perth|\n",
      "| -264344772|       Katherine|\n",
      "|-1945311707|   BadgerysCreek|\n",
      "|-1586257581|     MountGinini|\n",
      "| -575376651|           Uluru|\n",
      "|-1906226109|   SydneyAirport|\n",
      "| 2122068617|        Ballarat|\n",
      "|  155561671|           Cobar|\n",
      "| 1209160916|          Albury|\n",
      "|-1228765291|       Melbourne|\n",
      "| -730589816|      Townsville|\n",
      "|-2104817827|       Nuriootpa|\n",
      "|  561124118|            Nhil|\n",
      "| -340785775|     Tuggeranong|\n",
      "| 1106176135|          Albany|\n",
      "| -549198699|            Sale|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hash.toDF.show(df2.count().toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
